{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0697a04d",
   "metadata": {},
   "source": [
    "# utility.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1a87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 命令行参数\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run NGCF.\")\n",
    "    parser.add_argument('--weights_path', nargs='?', default='',\n",
    "                        help='Store model path.')\n",
    "    parser.add_argument('--data_path', nargs='?', default='Data/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--proj_path', nargs='?', default='',\n",
    "                        help='Project path.')\n",
    "\n",
    "    parser.add_argument('--dataset', nargs='?', default='gowalla',\n",
    "                        help='Choose a dataset from {gowalla, yelp2018, amazon-book}')\n",
    "    parser.add_argument('--pretrain', type=int, default=0,\n",
    "                        help='0: No pretrain, -1: Pretrain with the learned embeddings, 1:Pretrain with stored models.')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Interval of evaluation.')\n",
    "    parser.add_argument('--is_norm', type=int, default=1,\n",
    "                    help='Interval of evaluation.')\n",
    "    parser.add_argument('--epoch', type=int, default=1000,\n",
    "                        help='Number of epoch.')\n",
    "\n",
    "    parser.add_argument('--embed_size', type=int, default=64,\n",
    "                        help='Embedding size.')\n",
    "    parser.add_argument('--layer_size', nargs='?', default='[64, 64, 64, 64]',\n",
    "                        help='Output sizes of every layer')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                        help='Batch size.')\n",
    "\n",
    "    parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]',\n",
    "                        help='Regularizations.')\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='Learning rate.')\n",
    "\n",
    "    parser.add_argument('--model_type', nargs='?', default='lightgcn',\n",
    "                        help='Specify the name of model (lightgcn).')\n",
    "    parser.add_argument('--adj_type', nargs='?', default='pre',\n",
    "                        help='Specify the type of the adjacency (laplacian) matrix from {plain, norm, mean}.')\n",
    "    parser.add_argument('--alg_type', nargs='?', default='lightgcn',\n",
    "                        help='Specify the type of the graph convolutional layer from {ngcf, gcn, gcmc}.')\n",
    "\n",
    "    parser.add_argument('--gpu_id', type=int, default=0,\n",
    "                        help='0 for NAIS_prod, 1 for NAIS_concat')\n",
    "\n",
    "    parser.add_argument('--node_dropout_flag', type=int, default=0,\n",
    "                        help='0: Disable node dropout, 1: Activate node dropout')\n",
    "    parser.add_argument('--node_dropout', nargs='?', default='[0.1]',\n",
    "                        help='Keep probability w.r.t. node dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1]',\n",
    "                        help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "\n",
    "    parser.add_argument('--Ks', nargs='?', default='[20]',\n",
    "                        help='Top k(s) recommend')\n",
    "\n",
    "    parser.add_argument('--save_flag', type=int, default=0,\n",
    "                        help='0: Disable model saver, 1: Activate model saver')\n",
    "\n",
    "    parser.add_argument('--test_flag', nargs='?', default='part',\n",
    "                        help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n",
    "\n",
    "    parser.add_argument('--report', type=int, default=0,\n",
    "                        help='0: Disable performance report w.r.t. sparsity levels, 1: Show performance report w.r.t. sparsity levels')\n",
    "\n",
    "    # JpyNotebook 上报错，Pycharm上不会报错，必须传入args参数\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84aa74",
   "metadata": {},
   "source": [
    "# utility.batch_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18f134",
   "metadata": {},
   "source": [
    "# 使用的数据集\n",
    "\n",
    "## 可以在命令行中切换 gowalla，yelp2018，amazon-book 三个数据集，默认使用gowalla数据集\n",
    "## gowalla：美国微博好友关系数据集，每一条记录表示两两对应的朋友关系\n",
    "## yelp2018：美国商户店铺打分数据集，每一条包含用户对哪些商铺打了好评\n",
    "## amazon-book：亚马逊书籍评论数据集，每一条记录用户喜欢哪些书"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9501fa",
   "metadata": {},
   "source": [
    "# ·训练集：每一条代表某个用户对哪些商品更青睐\n",
    "# ·测试集：与训练集类似，不过其中未出现的商品会被认为用户讨厌这件商品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90832c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_score_matrix_foldout with python\n",
      "n_users=29858, n_items=40981\n",
      "n_interactions=1027370\n",
      "n_train=810128, n_test=217242, sparsity=0.00084\n"
     ]
    }
   ],
   "source": [
    "# from utility.parser import parse_args\n",
    "from utility.load_data import *\n",
    "from evaluator import eval_score_matrix_foldout\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import numpy as np\n",
    "cores = multiprocessing.cpu_count() // 2\n",
    "\n",
    "# ************************\n",
    "args = parse_args()\n",
    "\n",
    "# 创建数据集，默认数据集为gowalla\n",
    "data_generator = Data(path=args.data_path + args.dataset, batch_size=args.batch_size)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "\n",
    "\n",
    "# 给定test函数，用于测试推荐系统模型在给定用户集上的性能表现\n",
    "\"\"\"\n",
    "    sess：TensorFlow 会话对象；\n",
    "    model：推荐系统模型对象；\n",
    "    users_to_test：待测试的用户 ID 列表；\n",
    "    drop_flag：是否启用 Dropout，默认为 False；\n",
    "    train_set_flag：是否使用训练集作为测试集，默认为 0。\n",
    "\"\"\"\n",
    "def test(sess, model, users_to_test, drop_flag=False, train_set_flag=0):\n",
    "    # B: batch size\n",
    "    # N: the number of items\n",
    "    # top_show，表示要展示的，待排序的推荐列表\n",
    "    top_show = np.sort(model.Ks)\n",
    "    max_top = max(top_show)\n",
    "    # 将初值全部设为0，方便在测试中累加推荐列表\n",
    "    result = {'precision': np.zeros(len(model.Ks)), 'recall': np.zeros(len(model.Ks)), 'ndcg': np.zeros(len(model.Ks))}\n",
    "    \n",
    "    u_batch_size = BATCH_SIZE\n",
    "    \n",
    "    # 获取测试用户数量并分入若干个batch中\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "    \n",
    "    count = 0\n",
    "    all_result = []\n",
    "    # 获取商品列表\n",
    "    item_batch = range(ITEM_NUM)\n",
    "    # 遍历每个用户批次\n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "        # 设置批次起点与终点\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "        user_batch = test_users[start: end]\n",
    "        # 获取batch内的用户对所有物品的评分（预测数据）\n",
    "        if drop_flag == False:\n",
    "            rate_batch = sess.run(model.batch_ratings, {model.users: user_batch,\n",
    "                                                        model.pos_items: item_batch})\n",
    "        else:\n",
    "            rate_batch = sess.run(model.batch_ratings, {model.users: user_batch,\n",
    "                                                        model.pos_items: item_batch,\n",
    "                                                        # 以0.1，0.2，0.3...的dropout率变化（三层）\n",
    "                                                        model.node_dropout: [0.] * len(eval(args.layer_size)),\n",
    "                                                        model.mess_dropout: [0.] * len(eval(args.layer_size))})\n",
    "        rate_batch = np.array(rate_batch)# (B, N)\n",
    "        # 真实数据\n",
    "        test_items = []\n",
    "        # 如果使用的是测试集\n",
    "        if train_set_flag == 0:\n",
    "            # 遍历batch中的每个用户\n",
    "            for user in user_batch:\n",
    "                # 获取与用户有交互的商品id\n",
    "                test_items.append(data_generator.test_set[user])# (B, #test_items)\n",
    "                \n",
    "            \"\"\"\n",
    "            训练集和测试集是分开的，训练集包含了用户已经交互过的商品，而测试集包含了用户未曾交互过的商品。\n",
    "            在测试阶段，我们需要给用户推荐他们未曾交互过的商品，因此需要将训练集中的商品排除在推荐列表之外。\n",
    "            因此可以将训练集中的商品的评分设置为负无穷，这样它们就会被排在推荐列表的末尾。\n",
    "            如果使用训练集就不需要这一步。\n",
    "            \"\"\"\n",
    "            for idx, user in enumerate(user_batch):\n",
    "                    train_items_off = data_generator.train_items[user]\n",
    "                    rate_batch[idx][train_items_off] = -np.inf\n",
    "        # 如果使用的是训练集\n",
    "        else:\n",
    "            for user in user_batch:\n",
    "                test_items.append(data_generator.train_items[user])\n",
    "        # 利用评分矩阵获取最终结果，(B,k*metric_num), max_top= 20\n",
    "        batch_result = eval_score_matrix_foldout(rate_batch, test_items, max_top)\n",
    "        # 记录下本次处理了几个用户评分\n",
    "        count += len(batch_result)\n",
    "        all_result.append(batch_result)\n",
    "        \n",
    "    # 检测是否所有用户都完成打分\n",
    "    assert count == n_test_users\n",
    "    # 拼装所有批次的评估结果\n",
    "    all_result = np.concatenate(all_result, axis=0)\n",
    "    # 计算所有评估结果的平均值\n",
    "    final_result = np.mean(all_result, axis=0)  # mean\n",
    "    final_result = np.reshape(final_result, newshape=[5, max_top])\n",
    "    final_result = final_result[:, top_show-1]\n",
    "    final_result = np.reshape(final_result, newshape=[5, len(top_show)])\n",
    "    result['precision'] += final_result[0]\n",
    "    result['recall'] += final_result[1]\n",
    "    result['ndcg'] += final_result[3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上一步中使用的eval_score_matrix_foldout函数\n",
    "def eval_score_matrix_foldout(score_matrix, test_items, top_k=50, thread_num=None):\n",
    "    # 针对每个测试用户计算一组评估指标\n",
    "    def _eval_one_user(idx):\n",
    "        # 该用户的所有打分\n",
    "        scores = score_matrix[idx]  \n",
    "        # 该用户打分的所有物品\n",
    "        test_item = test_items[idx]  \n",
    "        \n",
    "        # 返回前K的评分最高的物品\n",
    "        ranking = argmax_top_k(scores, top_k)  # Top-K items\n",
    "        result = []\n",
    "        result.extend(precision(ranking, test_item)) # 精准率\n",
    "        result.extend(recall(ranking, test_item))    # 召回率\n",
    "        result.extend(map(ranking, test_item))       # 平均精确率\n",
    "        result.extend(ndcg(ranking, test_item))      # 归一化折损累计增益，评价排序效果\n",
    "        result.extend(mrr(ranking, test_item))       # 平均倒数排名，评价排序效果\n",
    "\n",
    "        result = np.array(result, dtype=np.float32).flatten()\n",
    "        return result\n",
    "    \n",
    "    # 多线程运算\n",
    "    with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
    "        batch_result = executor.map(_eval_one_user, range(len(test_items)))\n",
    "    \n",
    "    # 返回一个包含了所有测试用户评估结果的数组\n",
    "    result = list(batch_result)  # generator to list\n",
    "    return np.array(result)  # list to ndarray\n",
    "\n",
    "# 使用到的函数\n",
    "\"\"\"\n",
    "def argmax_top_k(a, top_k=50):\n",
    "    ele_idx = heapq.nlargest(top_k, zip(a, itertools.count()))\n",
    "    return np.array([idx for ele, idx in ele_idx], dtype=np.intc)\n",
    "\n",
    "def precision(rank, ground_truth):\n",
    "    hits = [1 if item in ground_truth else 0 for item in rank]\n",
    "    result = np.cumsum(hits, dtype=np.float)/np.arange(1, len(rank)+1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def recall(rank, ground_truth):\n",
    "    hits = [1 if item in ground_truth else 0 for item in rank]\n",
    "    result = np.cumsum(hits, dtype=np.float) / len(ground_truth)\n",
    "    return result\n",
    "\n",
    "\n",
    "def map(rank, ground_truth):\n",
    "    pre = precision(rank, ground_truth)\n",
    "    pre = [pre[idx] if item in ground_truth else 0 for idx, item in enumerate(rank)]\n",
    "    sum_pre = np.cumsum(pre, dtype=np.float32)\n",
    "    gt_len = len(ground_truth)\n",
    "    # len_rank = np.array([min(i, gt_len) for i in range(1, len(rank)+1)])\n",
    "    result = sum_pre/gt_len\n",
    "    return result\n",
    "\n",
    "\n",
    "def ndcg(rank, ground_truth):\n",
    "    len_rank = len(rank)\n",
    "    len_gt = len(ground_truth)\n",
    "    idcg_len = min(len_gt, len_rank)\n",
    "\n",
    "    # calculate idcg\n",
    "    idcg = np.cumsum(1.0 / np.log2(np.arange(2, len_rank + 2)))\n",
    "    idcg[idcg_len:] = idcg[idcg_len-1]\n",
    "\n",
    "    # idcg = np.cumsum(1.0/np.log2(np.arange(2, len_rank+2)))\n",
    "    dcg = np.cumsum([1.0/np.log2(idx+2) if item in ground_truth else 0.0 for idx, item in enumerate(rank)])\n",
    "    result = dcg/idcg\n",
    "    return result\n",
    "\n",
    "\n",
    "def mrr(rank, ground_truth):\n",
    "    last_idx = sys.maxsize\n",
    "    for idx, item in enumerate(rank):\n",
    "        if item in ground_truth:\n",
    "            last_idx = idx\n",
    "            break\n",
    "    result = np.zeros(len(rank), dtype=np.float32)\n",
    "    result[last_idx:] = 1.0/(last_idx+1)\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9574d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\SOFTWARE\\Anaconda\\envs\\sty\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import threading\n",
    "# Tensorflow版本为2.11.0，需要降到实验环境的1.11.0\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from utility.helper import *\n",
    "# from utility.batch_test import *\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "cpus = [x.name for x in device_lib.list_local_devices() if x.device_type == 'CPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838d3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(object):\n",
    "    def __init__(self, data_config, pretrain_data):\n",
    "        # 参数定义\n",
    "        self.model_type = 'LightGCN'# 模型类型\n",
    "        self.adj_type = args.adj_type # 邻接矩阵类型\n",
    "        self.alg_type = args.alg_type # 算法类型\n",
    "        self.pretrain_data = pretrain_data # 预处理数据（可选择加载）\n",
    "        self.n_users = data_config['n_users'] # 用户数量\n",
    "        self.n_items = data_config['n_items'] # 商品数量\n",
    "        self.n_fold = 100 # 数据集折数\n",
    "        self.norm_adj = data_config['norm_adj'] # 乘以度矩阵之后的邻接矩阵\n",
    "        self.n_nonzero_elems = self.norm_adj.count_nonzero() # norm_adj中非零元的数量\n",
    "        self.lr = args.lr # 学习率\n",
    "        self.emb_dim = args.embed_size # 嵌入维度\n",
    "        self.batch_size = args.batch_size # 批大小\n",
    "        self.weight_size = eval(args.layer_size) # 隐层权重尺寸\n",
    "        self.n_layers = len(self.weight_size) # 隐层数量\n",
    "        self.regs = eval(args.regs) # 正则化参数\n",
    "        self.decay = self.regs[0]   # L2正则化系数\n",
    "        self.log_dir=self.create_model_str() # 日志目录\n",
    "        self.verbose = args.verbose # 详细信息\n",
    "        self.Ks = eval(args.Ks) # 推荐商品列表\n",
    "\n",
    "\n",
    "        '''\n",
    "        *********************************************************\n",
    "        Create Placeholder for Input Data & Dropout.\n",
    "        '''\n",
    "        # 定义tensorflow中的占位符\n",
    "        self.users = tf.placeholder(tf.int32, shape=(None,)) \n",
    "        self.pos_items = tf.placeholder(tf.int32, shape=(None,)) # 正物品\n",
    "        self.neg_items = tf.placeholder(tf.int32, shape=(None,)) # 负物品\n",
    "        \n",
    "        self.node_dropout_flag = args.node_dropout_flag\n",
    "        self.node_dropout = tf.placeholder(tf.float32, shape=[None]) # 节点和消息权重的dropout\n",
    "        self.mess_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "        with tf.name_scope('TRAIN_LOSS'):\n",
    "            \"\"\"\n",
    "            self.train_loss：训练总损失\n",
    "            self.train_mf_loss：损失函数\n",
    "            self.train_emb_loss：正则化项\n",
    "            self.train_reg_loss：常数0，可以和其他损失函数拼接，防止维度不匹配\n",
    "            \"\"\"\n",
    "            self.train_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('train_loss', self.train_loss)\n",
    "            self.train_mf_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('train_mf_loss', self.train_mf_loss)\n",
    "            self.train_emb_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('train_emb_loss', self.train_emb_loss)\n",
    "            self.train_reg_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('train_reg_loss', self.train_reg_loss)\n",
    "        # 张量合并\n",
    "        self.merged_train_loss = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES, 'TRAIN_LOSS'))\n",
    "        \n",
    "        \n",
    "        with tf.name_scope('TRAIN_ACC'):\n",
    "            \"\"\"\n",
    "            self.train_rec_first：最推荐位置的召回率\n",
    "            self.train_rec_last：最不推荐位置的召回率\n",
    "            \"\"\"\n",
    "            self.train_rec_first = tf.placeholder(tf.float32)\n",
    "            #record for top(Ks[0])\n",
    "            tf.summary.scalar('train_rec_first', self.train_rec_first)\n",
    "            self.train_rec_last = tf.placeholder(tf.float32)\n",
    "            #record for top(Ks[-1])\n",
    "            tf.summary.scalar('train_rec_last', self.train_rec_last)\n",
    "            self.train_ndcg_first = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('train_ndcg_first', self.train_ndcg_first)\n",
    "            self.train_ndcg_last = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('train_ndcg_last', self.train_ndcg_last)\n",
    "        self.merged_train_acc = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES, 'TRAIN_ACC'))\n",
    "\n",
    "        with tf.name_scope('TEST_LOSS'):\n",
    "            \"\"\"\n",
    "            self.test_loss：测试总损失\n",
    "            self.test_mf_loss：测试MF损失\n",
    "            self.test_emb_loss：测试嵌入损失\n",
    "            self.test_reg_loss：测试正则化损失\n",
    "            \n",
    "            \"\"\"\n",
    "            self.test_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_loss', self.test_loss)\n",
    "            self.test_mf_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_mf_loss', self.test_mf_loss)\n",
    "            self.test_emb_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_emb_loss', self.test_emb_loss)\n",
    "            self.test_reg_loss = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_reg_loss', self.test_reg_loss)\n",
    "        self.merged_test_loss = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES, 'TEST_LOSS'))\n",
    "\n",
    "        with tf.name_scope('TEST_ACC'):\n",
    "            # 同理\n",
    "            self.test_rec_first = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_rec_first', self.test_rec_first)\n",
    "            self.test_rec_last = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_rec_last', self.test_rec_last)\n",
    "            self.test_ndcg_first = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_ndcg_first', self.test_ndcg_first)\n",
    "            self.test_ndcg_last = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('test_ndcg_last', self.test_ndcg_last)\n",
    "        self.merged_test_acc = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES, 'TEST_ACC'))\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Create Model Parameters (i.e., Initialize Weights).\n",
    "        \"\"\"\n",
    "        # 初始化权重\n",
    "        self.weights = self._init_weights()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Compute Graph-based Representations of all users & items via Message-Passing Mechanism of Graph Neural Networks.\n",
    "        Different Convolutional Layers:\n",
    "            1. ngcf: defined in 'Neural Graph Collaborative Filtering', SIGIR2019;\n",
    "            2. gcn:  defined in 'Semi-Supervised Classification with Graph Convolutional Networks', ICLR2018;\n",
    "            3. gcmc: defined in 'Graph Convolutional Matrix Completion', KDD2018;\n",
    "        \"\"\"\n",
    "        \n",
    "        # 根据之前代码中的定义来确定模型、算法类型，可选四种模型\n",
    "        if self.alg_type in ['lightgcn']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_lightgcn_embed()\n",
    "            \n",
    "        elif self.alg_type in ['ngcf']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_ngcf_embed()\n",
    "\n",
    "        elif self.alg_type in ['gcn']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_gcn_embed()\n",
    "\n",
    "        elif self.alg_type in ['gcmc']:\n",
    "            self.ua_embeddings, self.ia_embeddings = self._create_gcmc_embed()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Establish the final representations for user-item pairs in batch.\n",
    "        \"\"\"\n",
    "        # 获取嵌入向量，类似于Forward函数\n",
    "        self.u_g_embeddings = tf.nn.embedding_lookup(self.ua_embeddings, self.users)\n",
    "        self.pos_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.pos_items)\n",
    "        self.neg_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.neg_items)\n",
    "        self.u_g_embeddings_pre = tf.nn.embedding_lookup(self.weights['user_embedding'], self.users)\n",
    "        self.pos_i_g_embeddings_pre = tf.nn.embedding_lookup(self.weights['item_embedding'], self.pos_items)\n",
    "        self.neg_i_g_embeddings_pre = tf.nn.embedding_lookup(self.weights['item_embedding'], self.neg_items)\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Inference for the testing phase.\n",
    "        \"\"\"\n",
    "        # 计算内积得分，将用户向量和物品向量作内积，数值越大的两个向量越相似，反映物品更适合该用户\n",
    "        self.batch_ratings = tf.matmul(self.u_g_embeddings, self.pos_i_g_embeddings, transpose_a=False, transpose_b=True)\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Generate Predictions & Optimize via BPR loss.\n",
    "        \"\"\"\n",
    "        # 定义损失函数(贝叶斯排序)和优化器(Adam)\n",
    "        self.mf_loss, self.emb_loss, self.reg_loss = self.create_bpr_loss(self.u_g_embeddings,\n",
    "                                                                          self.pos_i_g_embeddings,\n",
    "                                                                          self.neg_i_g_embeddings)\n",
    "        self.loss = self.mf_loss + self.emb_loss\n",
    "\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "    \n",
    "    # 记录模型路径\n",
    "    def create_model_str(self):\n",
    "        log_dir = '/' + self.alg_type+'/layers_'+str(self.n_layers)+'/dim_'+str(self.emb_dim)\n",
    "        log_dir+='/'+args.dataset+'/lr_' + str(self.lr) + '/reg_' + str(self.decay)\n",
    "        return log_dir\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    LightGCN中，训练的对象其实仅是初始化的数据嵌入E0\n",
    "    \"\"\"\n",
    "    # 初始化权重函数\n",
    "    def _init_weights(self):\n",
    "        all_weights = dict() \n",
    "        initializer = tf.random_normal_initializer(stddev=0.01) #tf.contrib.layers.xavier_initializer()\n",
    "        # 是否使用预训练数据\n",
    "        if self.pretrain_data is None:\n",
    "            # 随机数据初始嵌入\n",
    "            all_weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n",
    "            all_weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n",
    "            print('using random initialization')#print('using xavier initialization')\n",
    "        else:\n",
    "            # 预训练数据初始嵌入\n",
    "            all_weights['user_embedding'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
    "                                                        name='user_embedding', dtype=tf.float32)\n",
    "            all_weights['item_embedding'] = tf.Variable(initial_value=self.pretrain_data['item_embed'], trainable=True,\n",
    "                                                        name='item_embedding', dtype=tf.float32)\n",
    "            print('using pretrained initialization')\n",
    "            \n",
    "        self.weight_size_list = [self.emb_dim] + self.weight_size\n",
    "        \n",
    "        # 初始化若干矩阵，图卷积、双向网络、mlp，供上文中的四种不同模型使用（LightGCN不需要权重矩阵，因此只有三个）\n",
    "        for k in range(self.n_layers):\n",
    "            all_weights['W_gc_%d' %k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
    "            all_weights['b_gc_%d' %k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
    "\n",
    "            all_weights['W_bi_%d' % k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
    "            all_weights['b_bi_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k + 1]]), name='b_bi_%d' % k)\n",
    "\n",
    "            all_weights['W_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_mlp_%d' % k)\n",
    "            all_weights['b_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_mlp_%d' % k)\n",
    "\n",
    "        return all_weights\n",
    "    \n",
    "    # 将原始的邻接矩阵分割成若干个子矩阵，减少资源浪费，提高训练速度\n",
    "    def _split_A_hat(self, X):\n",
    "        A_fold_hat = []\n",
    "        \n",
    "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold -1:\n",
    "                end = self.n_users + self.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            # 转为Tensorflow的稀疏张量\n",
    "            A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "        return A_fold_hat\n",
    "    \n",
    "    # 进一步加入dropout\n",
    "    def _split_A_hat_node_dropout(self, X):\n",
    "        A_fold_hat = []\n",
    "\n",
    "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold -1:\n",
    "                end = self.n_users + self.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            temp = self._convert_sp_mat_to_sp_tensor(X[start:end])\n",
    "            n_nonzero_temp = X[start:end].count_nonzero()\n",
    "            A_fold_hat.append(self._dropout_sparse(temp, 1 - self.node_dropout[0], n_nonzero_temp))\n",
    "\n",
    "        return A_fold_hat\n",
    "\n",
    "    # LightGCN 的嵌入层运算\n",
    "    def _create_lightgcn_embed(self):\n",
    "        # 切分子矩阵\n",
    "        if self.node_dropout_flag:\n",
    "            A_fold_hat = self._split_A_hat_node_dropout(self.norm_adj)\n",
    "        else:\n",
    "            A_fold_hat = self._split_A_hat(self.norm_adj)\n",
    "        \n",
    "        # self.weights中的表示，将user_embedding和item_embedding拼接\n",
    "        ego_embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "        \n",
    "        for k in range(0, self.n_layers):\n",
    "\n",
    "            temp_embed = []\n",
    "            # 将切分的稀疏张量与密集张量相乘,其实结果就是A度×嵌入,得到更新后的E\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "            # 再将所有向量拼接起来，拼接完成后的形状为(n_users + n_items) × emb_dim\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "            ego_embeddings = side_embeddings\n",
    "            all_embeddings += [ego_embeddings]\n",
    "        # 按照第1维度拼接\n",
    "        all_embeddings=tf.stack(all_embeddings,1)\n",
    "        # 嵌入均值\n",
    "        all_embeddings=tf.reduce_mean(all_embeddings,axis=1,keepdims=False)\n",
    "        # 拆分为用户的嵌入向量和物品的嵌入向量\n",
    "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
    "        return u_g_embeddings, i_g_embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    def _create_ngcf_embed(self):\n",
    "        if self.node_dropout_flag:\n",
    "            A_fold_hat = self._split_A_hat_node_dropout(self.norm_adj)\n",
    "        else:\n",
    "            A_fold_hat = self._split_A_hat(self.norm_adj)\n",
    "\n",
    "        ego_embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(0, self.n_layers):\n",
    "\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "            sum_embeddings = tf.nn.leaky_relu(tf.matmul(side_embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n",
    "\n",
    "\n",
    "\n",
    "            # bi messages of neighbors.\n",
    "            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n",
    "            # transformed bi messages of neighbors.\n",
    "            bi_embeddings = tf.nn.leaky_relu(tf.matmul(bi_embeddings, self.weights['W_bi_%d' % k]) + self.weights['b_bi_%d' % k])\n",
    "            # non-linear activation.\n",
    "            ego_embeddings = sum_embeddings + bi_embeddings\n",
    "\n",
    "            # message dropout.\n",
    "            # ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - self.mess_dropout[k])\n",
    "\n",
    "            # normalize the distribution of embeddings.\n",
    "            norm_embeddings = tf.nn.l2_normalize(ego_embeddings, axis=1)\n",
    "\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = tf.concat(all_embeddings, 1)\n",
    "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
    "        return u_g_embeddings, i_g_embeddings\n",
    "    \n",
    "    \n",
    "    def _create_gcn_embed(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj)\n",
    "        embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
    "\n",
    "\n",
    "        all_embeddings = [embeddings]\n",
    "\n",
    "        for k in range(0, self.n_layers):\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings))\n",
    "\n",
    "            embeddings = tf.concat(temp_embed, 0)\n",
    "            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.weights['W_gc_%d' %k]) + self.weights['b_gc_%d' %k])\n",
    "            # embeddings = tf.nn.dropout(embeddings, 1 - self.mess_dropout[k])\n",
    "\n",
    "            all_embeddings += [embeddings]\n",
    "\n",
    "        all_embeddings = tf.concat(all_embeddings, 1)\n",
    "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
    "        return u_g_embeddings, i_g_embeddings\n",
    "    \n",
    "    def _create_gcmc_embed(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj)\n",
    "\n",
    "        embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
    "\n",
    "        all_embeddings = []\n",
    "\n",
    "        for k in range(0, self.n_layers):\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings))\n",
    "            embeddings = tf.concat(temp_embed, 0)\n",
    "            # convolutional layer.\n",
    "            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n",
    "            # dense layer.\n",
    "            mlp_embeddings = tf.matmul(embeddings, self.weights['W_mlp_%d' %k]) + self.weights['b_mlp_%d' %k]\n",
    "            # mlp_embeddings = tf.nn.dropout(mlp_embeddings, 1 - self.mess_dropout[k])\n",
    "\n",
    "            all_embeddings += [mlp_embeddings]\n",
    "        all_embeddings = tf.concat(all_embeddings, 1)\n",
    "\n",
    "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
    "        return u_g_embeddings, i_g_embeddings\n",
    "    \"\"\"\n",
    "    # 损失函数 贝叶斯个性化排序\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        # 获取正样本喜好度和负样本喜好度\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1)\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)\n",
    "        \n",
    "        regularizer = tf.nn.l2_loss(self.u_g_embeddings_pre) + tf.nn.l2_loss(\n",
    "                self.pos_i_g_embeddings_pre) + tf.nn.l2_loss(self.neg_i_g_embeddings_pre)\n",
    "        regularizer = regularizer / self.batch_size\n",
    "        \n",
    "        # softplus(x) = log(1 + exp(x))\n",
    "        mf_loss = tf.reduce_mean(tf.nn.softplus(-(pos_scores - neg_scores)))\n",
    "        \n",
    "\n",
    "        emb_loss = self.decay * regularizer\n",
    "\n",
    "        reg_loss = tf.constant(0.0, tf.float32, [1])\n",
    "\n",
    "        return mf_loss, emb_loss, reg_loss\n",
    "    \n",
    "    # 将稀疏矩阵转为 TensorFlow 中的稀疏张量，以便在TensorFlow 中进行计算\n",
    "    \"\"\"\n",
    "    coo对象：稀疏矩阵，每一条的格式为 （行 列 非零值）\n",
    "    indices：仅取出非零值的位置\n",
    "    SparseTensor：稀疏张量，存储为：（indices values dense_shape）\n",
    "    数据格式例：\n",
    "    indices = [[0, 1], [1, 0], [2, 2], [2, 3]]  # 4个非零元素的位置\n",
    "    values = [1, 2, 3, 4]  # 4个非零元素的值\n",
    "    dense_shape = [3, 4]  # 张量的形状为3行4列\n",
    "    \"\"\"\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    \"\"\"\n",
    "    def _dropout_sparse(self, X, keep_prob, n_nonzero_elems):\n",
    "        \n",
    "        # Dropout for sparse tensors.\n",
    "        \n",
    "        noise_shape = [n_nonzero_elems]\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random_uniform(noise_shape)\n",
    "        dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "        pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "\n",
    "        return pre_out * tf.div(1., keep_prob)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c590ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预训练的数据（前提要有预训练的数据）\n",
    "def load_pretrained_data():\n",
    "    pretrain_path = '%spretrain/%s/%s.npz' % (args.proj_path, args.dataset, 'embedding')\n",
    "    try:\n",
    "        pretrain_data = np.load(pretrain_path)\n",
    "        print('load the pretrained embeddings.')\n",
    "    except Exception:\n",
    "        pretrain_data = None\n",
    "    return pretrain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5378307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU并行处理\n",
    "# parallelized sampling on CPU \n",
    "class sample_thread(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        with tf.device(cpus[0]):\n",
    "            self.data = data_generator.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0488e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_thread_test(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        with tf.device(cpus[0]):\n",
    "            self.data = data_generator.sample_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea30073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU并行处理\n",
    "# training on GPU\n",
    "class train_thread(threading.Thread):\n",
    "    def __init__(self,model, sess, sample):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        self.sample = sample\n",
    "    def run(self):\n",
    "#         with tf.device(gpus[1]):\n",
    "#             with tf.degice('/CPU:0'):\n",
    "        users, pos_items, neg_items = self.sample.data\n",
    "        self.data = sess.run([self.model.opt, self.model.loss, self.model.mf_loss, self.model.emb_loss, self.model.reg_loss],\n",
    "                                feed_dict={model.users: users, model.pos_items: pos_items,\n",
    "                                            model.node_dropout: eval(args.node_dropout),\n",
    "                                            model.mess_dropout: eval(args.mess_dropout),\n",
    "                                            model.neg_items: neg_items})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "308b89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_thread_test(threading.Thread):\n",
    "    def __init__(self,model, sess, sample):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        self.sample = sample\n",
    "    def run(self):\n",
    "#         with tf.device(gpus[1]):\n",
    "        users, pos_items, neg_items = self.sample.data\n",
    "        self.data = sess.run([self.model.loss, self.model.mf_loss, self.model.emb_loss],\n",
    "                            feed_dict={model.users: users, model.pos_items: pos_items,\n",
    "                                    model.neg_items: neg_items,\n",
    "                                    model.node_dropout: eval(args.node_dropout),\n",
    "                                    model.mess_dropout: eval(args.mess_dropout)})   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9714851",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c278d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "f0 = time()\n",
    "\n",
    "config = dict()\n",
    "config['n_users'] = data_generator.n_users\n",
    "config['n_items'] = data_generator.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97bf8387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (70839, 70839) 0.15360474586486816\n",
      "use the pre adjcency matrix\n",
      "using random initialization\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "*********************************************************\n",
    "Generate the Laplacian matrix, where each entry defines the decay factor (e.g., p_ui) between two connected nodes.\n",
    "\"\"\"\n",
    "# 获取邻接矩阵\n",
    "plain_adj, norm_adj, mean_adj,pre_adj = data_generator.get_adj_mat()\n",
    "if args.adj_type == 'plain':\n",
    "    config['norm_adj'] = plain_adj\n",
    "    print('use the plain adjacency matrix')\n",
    "elif args.adj_type == 'norm':\n",
    "    config['norm_adj'] = norm_adj\n",
    "    print('use the normalized adjacency matrix')\n",
    "elif args.adj_type == 'gcmc':\n",
    "    config['norm_adj'] = mean_adj\n",
    "    print('use the gcmc adjacency matrix')\n",
    "elif args.adj_type=='pre':\n",
    "    config['norm_adj']=pre_adj\n",
    "    print('use the pre adjcency matrix')\n",
    "else:\n",
    "    config['norm_adj'] = mean_adj + sp.eye(mean_adj.shape[0])\n",
    "    print('use the mean adjacency matrix')\n",
    "t0 = time()\n",
    "if args.pretrain == -1:\n",
    "    pretrain_data = load_pretrained_data()\n",
    "else:\n",
    "    pretrain_data = None\n",
    "model = LightGCN(data_config=config, pretrain_data=pretrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d139d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "\"\"\"\n",
    "*********************************************************\n",
    "Save the model parameters.\n",
    "\"\"\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if args.save_flag == 1:\n",
    "    layer = '-'.join([str(l) for l in eval(args.layer_size)])\n",
    "    weights_save_path = '%sweights/%s/%s/%s/l%s_r%s' % (args.weights_path, args.dataset, model.model_type, layer,\n",
    "                                                        str(args.lr), '-'.join([str(r) for r in eval(args.regs)]))\n",
    "    ensureDir(weights_save_path)\n",
    "    save_saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96edd6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without pretraining.\n"
     ]
    }
   ],
   "source": [
    "# 读取模型参数\n",
    "\"\"\"\n",
    "*********************************************************\n",
    "Reload the pretrained model parameters.\n",
    "\"\"\"\n",
    "if args.pretrain == 1:\n",
    "    layer = '-'.join([str(l) for l in eval(args.layer_size)])\n",
    "\n",
    "    pretrain_path = '%sweights/%s/%s/%s/l%s_r%s' % (args.weights_path, args.dataset, model.model_type, layer,\n",
    "                                                    str(args.lr), '-'.join([str(r) for r in eval(args.regs)]))\n",
    "\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_path + '/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('load the pretrained model parameters from: ', pretrain_path)\n",
    "\n",
    "        # *********************************************************\n",
    "        # get the performance from pretrained model.\n",
    "        if args.report != 1:\n",
    "            users_to_test = list(data_generator.test_set.keys())\n",
    "            ret = test(sess, model, users_to_test, drop_flag=True)\n",
    "            cur_best_pre_0 = ret['recall'][0]\n",
    "\n",
    "            pretrain_ret = 'pretrained model recall=[%s], precision=[%s], '\\\n",
    "                           'ndcg=[%s]' % \\\n",
    "                           (', '.join(['%.5f' % r for r in ret['recall']]),\n",
    "                            ', '.join(['%.5f' % r for r in ret['precision']]),\n",
    "                            ', '.join(['%.5f' % r for r in ret['ndcg']]))\n",
    "            print(pretrain_ret)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        cur_best_pre_0 = 0.\n",
    "        print('without pretraining.')\n",
    "\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cur_best_pre_0 = 0.\n",
    "    print('without pretraining.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*********************************************************\n",
    "Get the performance w.r.t. different sparsity levels.\n",
    "\"\"\"\n",
    "if args.report == 1:\n",
    "    assert args.test_flag == 'full'\n",
    "    users_to_test_list, split_state = data_generator.get_sparsity_split()\n",
    "    users_to_test_list.append(list(data_generator.test_set.keys()))\n",
    "    split_state.append('all')\n",
    "\n",
    "    report_path = '%sreport/%s/%s.result' % (args.proj_path, args.dataset, model.model_type)\n",
    "    ensureDir(report_path)\n",
    "    f = open(report_path, 'w')\n",
    "    f.write(\n",
    "        'embed_size=%d, lr=%.4f, layer_size=%s, keep_prob=%s, regs=%s, loss_type=%s, adj_type=%s\\n'\n",
    "        % (args.embed_size, args.lr, args.layer_size, args.keep_prob, args.regs, args.loss_type, args.adj_type))\n",
    "\n",
    "    for i, users_to_test in enumerate(users_to_test_list):\n",
    "        ret = test(sess, model, users_to_test, drop_flag=True)\n",
    "\n",
    "        final_perf = \"recall=[%s], precision=[%s], ndcg=[%s]\" % \\\n",
    "                     (', '.join(['%.5f' % r for r in ret['recall']]),\n",
    "                      ', '.join(['%.5f' % r for r in ret['precision']]),\n",
    "                      ', '.join(['%.5f' % r for r in ret['ndcg']]))\n",
    "\n",
    "        f.write('\\t%s\\n\\t%s\\n' % (split_state[i], final_perf))\n",
    "    f.close()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*********************************************************\n",
    "Train.\n",
    "\"\"\"\n",
    "tensorboard_model_path = 'tensorboard/'\n",
    "if not os.path.exists(tensorboard_model_path):\n",
    "    os.makedirs(tensorboard_model_path)\n",
    "run_time = 1\n",
    "while (True):\n",
    "    if os.path.exists(tensorboard_model_path + model.log_dir +'/run_' + str(run_time)):\n",
    "        run_time += 1\n",
    "    else:\n",
    "        break\n",
    "train_writer = tf.summary.FileWriter(tensorboard_model_path +model.log_dir+ '/run_' + str(run_time), sess.graph)\n",
    "\n",
    "\n",
    "loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n",
    "stopping_step = 0\n",
    "should_stop = False\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    print(\"Epoch:\",epoch)\n",
    "    t1 = time()\n",
    "    loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
    "    n_batch = data_generator.n_train // args.batch_size + 1\n",
    "    loss_test,mf_loss_test,emb_loss_test,reg_loss_test=0.,0.,0.,0.\n",
    "    '''\n",
    "    *********************************************************\n",
    "    parallelized sampling\n",
    "    '''\n",
    "    sample_last = sample_thread()\n",
    "    sample_last.start()\n",
    "    sample_last.join()\n",
    "    for idx in range(n_batch):\n",
    "        train_cur = train_thread(model, sess, sample_last)\n",
    "        sample_next = sample_thread()\n",
    "\n",
    "        train_cur.start()\n",
    "        sample_next.start()\n",
    "\n",
    "        sample_next.join()\n",
    "        train_cur.join()\n",
    "        print('r')\n",
    "        users, pos_items, neg_items = sample_last.data\n",
    "        _, batch_loss, batch_mf_loss, batch_emb_loss, batch_reg_loss = train_cur.data\n",
    "        sample_last = sample_next\n",
    "\n",
    "        loss += batch_loss/n_batch\n",
    "        mf_loss += batch_mf_loss/n_batch\n",
    "        emb_loss += batch_emb_loss/n_batch\n",
    "\n",
    "    summary_train_loss= sess.run(model.merged_train_loss,\n",
    "                                  feed_dict={model.train_loss: loss, model.train_mf_loss: mf_loss,\n",
    "                                             model.train_emb_loss: emb_loss, model.train_reg_loss: reg_loss})\n",
    "    train_writer.add_summary(summary_train_loss, epoch)\n",
    "    if np.isnan(loss) == True:\n",
    "        print('ERROR: loss is nan.')\n",
    "        sys.exit()\n",
    "\n",
    "    if (epoch % 20) != 0:\n",
    "        if args.verbose > 0 and epoch % args.verbose == 0:\n",
    "            perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (\n",
    "                epoch, time() - t1, loss, mf_loss, emb_loss)\n",
    "            print(perf_str)\n",
    "        continue\n",
    "        \n",
    "    users_to_test = list(data_generator.train_items.keys())\n",
    "    ret = test(sess, model, users_to_test ,drop_flag=True,train_set_flag=1)\n",
    "    perf_str = 'Epoch %d: train==[%.5f=%.5f + %.5f + %.5f], recall=[%s], precision=[%s], ndcg=[%s]' % \\\n",
    "               (epoch, loss, mf_loss, emb_loss, reg_loss, \n",
    "                ', '.join(['%.5f' % r for r in ret['recall']]),\n",
    "                ', '.join(['%.5f' % r for r in ret['precision']]),\n",
    "                ', '.join(['%.5f' % r for r in ret['ndcg']]))\n",
    "    print(perf_str)\n",
    "    summary_train_acc = sess.run(model.merged_train_acc, feed_dict={model.train_rec_first: ret['recall'][0],\n",
    "                                                                    model.train_rec_last: ret['recall'][-1],\n",
    "                                                                    model.train_ndcg_first: ret['ndcg'][0],\n",
    "                                                                    model.train_ndcg_last: ret['ndcg'][-1]})\n",
    "    train_writer.add_summary(summary_train_acc, epoch // 20)\n",
    "\n",
    "    '''\n",
    "    *********************************************************\n",
    "    parallelized sampling\n",
    "    '''\n",
    "    sample_last= sample_thread_test()\n",
    "    sample_last.start()\n",
    "    sample_last.join()\n",
    "    for idx in range(n_batch):\n",
    "        train_cur = train_thread_test(model, sess, sample_last)\n",
    "        sample_next = sample_thread_test()\n",
    "\n",
    "        train_cur.start()\n",
    "        sample_next.start()\n",
    "\n",
    "        sample_next.join()\n",
    "        train_cur.join()\n",
    "\n",
    "        users, pos_items, neg_items = sample_last.data\n",
    "        batch_loss_test, batch_mf_loss_test, batch_emb_loss_test = train_cur.data\n",
    "        sample_last = sample_next\n",
    "\n",
    "        loss_test += batch_loss_test / n_batch\n",
    "        mf_loss_test += batch_mf_loss_test / n_batch\n",
    "        emb_loss_test += batch_emb_loss_test / n_batch\n",
    "\n",
    "    summary_test_loss = sess.run(model.merged_test_loss,\n",
    "                                 feed_dict={model.test_loss: loss_test, model.test_mf_loss: mf_loss_test,\n",
    "                                            model.test_emb_loss: emb_loss_test, model.test_reg_loss: reg_loss_test})\n",
    "    train_writer.add_summary(summary_test_loss, epoch // 20)\n",
    "    t2 = time()\n",
    "    users_to_test = list(data_generator.test_set.keys())\n",
    "    ret = test(sess, model, users_to_test, drop_flag=True)\n",
    "    summary_test_acc = sess.run(model.merged_test_acc,\n",
    "                                feed_dict={model.test_rec_first: ret['recall'][0], model.test_rec_last: ret['recall'][-1],\n",
    "                                           model.test_ndcg_first: ret['ndcg'][0], model.test_ndcg_last: ret['ndcg'][-1]})\n",
    "    train_writer.add_summary(summary_test_acc, epoch // 20)\n",
    "\n",
    "\n",
    "    t3 = time()\n",
    "\n",
    "    loss_loger.append(loss)\n",
    "    rec_loger.append(ret['recall'])\n",
    "    pre_loger.append(ret['precision'])\n",
    "    ndcg_loger.append(ret['ndcg'])\n",
    "\n",
    "    if args.verbose > 0:\n",
    "        perf_str = 'Epoch %d [%.1fs + %.1fs]: test==[%.5f=%.5f + %.5f + %.5f], recall=[%s], ' \\\n",
    "                   'precision=[%s], ndcg=[%s]' % \\\n",
    "                   (epoch, t2 - t1, t3 - t2, loss_test, mf_loss_test, emb_loss_test, reg_loss_test, \n",
    "                    ', '.join(['%.5f' % r for r in ret['recall']]),\n",
    "                    ', '.join(['%.5f' % r for r in ret['precision']]),\n",
    "                    ', '.join(['%.5f' % r for r in ret['ndcg']]))\n",
    "        print(perf_str)\n",
    "\n",
    "    cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n",
    "                                                                stopping_step, expected_order='acc', flag_step=5)\n",
    "\n",
    "    # *********************************************************\n",
    "    # early stopping when cur_best_pre_0 is decreasing for ten successive steps.\n",
    "    if should_stop == True:\n",
    "        break\n",
    "\n",
    "    # *********************************************************\n",
    "    # save the user & item embeddings for pretraining.\n",
    "    if ret['recall'][0] == cur_best_pre_0 and args.save_flag == 1:\n",
    "        save_saver.save(sess, weights_save_path + '/weights', global_step=epoch)\n",
    "        print('save the weights in path: ', weights_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = np.array(rec_loger)\n",
    "pres = np.array(pre_loger)\n",
    "ndcgs = np.array(ndcg_loger)\n",
    "\n",
    "best_rec_0 = max(recs[:, 0])\n",
    "idx = list(recs[:, 0]).index(best_rec_0)\n",
    "\n",
    "final_perf = \"Best Iter=[%d]@[%.1f]\\trecall=[%s], precision=[%s], ndcg=[%s]\" % \\\n",
    "             (idx, time() - t0, '\\t'.join(['%.5f' % r for r in recs[idx]]),\n",
    "              '\\t'.join(['%.5f' % r for r in pres[idx]]),\n",
    "              '\\t'.join(['%.5f' % r for r in ndcgs[idx]]))\n",
    "print(final_perf)\n",
    "\n",
    "save_path = '%soutput/%s/%s.result' % (args.proj_path, args.dataset, model.model_type)\n",
    "ensureDir(save_path)\n",
    "f = open(save_path, 'a')\n",
    "\n",
    "f.write(\n",
    "    'embed_size=%d, lr=%.4f, layer_size=%s, node_dropout=%s, mess_dropout=%s, regs=%s, adj_type=%s\\n\\t%s\\n'\n",
    "    % (args.embed_size, args.lr, args.layer_size, args.node_dropout, args.mess_dropout, args.regs,\n",
    "       args.adj_type, final_perf))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sty]",
   "language": "python",
   "name": "conda-env-sty-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
